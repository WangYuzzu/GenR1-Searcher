import os
import json
import torch
import requests
import re
import copy
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from datasets import Dataset
from torch.utils.data import DataLoader
from collections import defaultdict
from tqdm import tqdm
import argparse
import time
import multiprocessing as mp
from typing import List, Dict, Any
from functools import partial


class TestDataset:
    """æµ‹è¯•æ•°æ®é›†ç±»"""

    def __init__(self, jsonl_file):
        self.data = []

        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                self.data.append(item)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def load_rl_model(checkpoint_path, base_model_path=None, device='cuda', gpu_memory_rate=0.9, tensor_parallel_size=1):
    """
    åŠ è½½RLè®­ç»ƒçš„æ¨¡å‹

    Args:
        checkpoint_path: checkpointè·¯å¾„ï¼ˆå¯èƒ½æ˜¯global_step300è¿™æ ·çš„å­ç›®å½•ï¼‰
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå½“checkpointåªåŒ…å«æƒé‡æ—¶ä½¿ç”¨ï¼‰
        device: è®¾å¤‡
        gpu_memory_rate: GPUå†…å­˜ä½¿ç”¨ç‡
        tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°

    Returns:
        model, tokenizer
    """
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ä»: {checkpoint_path}")
    print(f"GPUé…ç½®: {tensor_parallel_size}å¼ å¡å¹¶è¡Œï¼Œå†…å­˜åˆ©ç”¨ç‡{gpu_memory_rate}")

    # æ£€æŸ¥checkpointç›®å½•å†…å®¹
    if os.path.isdir(checkpoint_path):
        files = os.listdir(checkpoint_path)
        print(f"ğŸ“ Checkpointç›®å½•åŒ…å«: {files}")

        # æ£€æŸ¥æ˜¯å¦æœ‰.ptæˆ–.binæ–‡ä»¶
        weight_files = [f for f in files if f.endswith('.pt') or f.endswith('.bin') or f.endswith('.safetensors')]
        if weight_files:
            print(f"ğŸ” æ‰¾åˆ°æƒé‡æ–‡ä»¶: {weight_files}")

    # æ£€æŸ¥æ˜¯å¦æ˜¯RLè®­ç»ƒçš„checkpointå­ç›®å½•ï¼ˆå¦‚global_step300ï¼‰
    if not os.path.exists(os.path.join(checkpoint_path, "config.json")):
        print(f"âš ï¸ {checkpoint_path} æ²¡æœ‰config.jsonï¼Œçœ‹èµ·æ¥æ˜¯RLè®­ç»ƒçš„æƒé‡ç›®å½•")

        # å¿…é¡»æœ‰åŸºç¡€æ¨¡å‹è·¯å¾„
        if not base_model_path:
            print(f"âŒ æ£€æµ‹åˆ°RLæƒé‡æ–‡ä»¶ï¼Œä½†æ²¡æœ‰æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„")
            print("è¯·ä½¿ç”¨ --base_model_path å‚æ•°æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚ï¼š")
            print("  --base_model_path /root/autodl-tmp/Qwen-2.5-3B-Instruct")
            print("  --base_model_path Qwen/Qwen2.5-7B-Instruct")
            raise ValueError("RLæƒé‡éœ€è¦æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„")

        if not os.path.exists(base_model_path):
            print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
            raise ValueError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")

        print(f"âœ… ä½¿ç”¨åŸºç¡€æ¨¡å‹: {base_model_path}")

        # åŠ è½½tokenizerä»åŸºç¡€æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥å­˜æ”¾åˆå¹¶åçš„æ¨¡å‹
        import tempfile
        import shutil

        temp_model_dir = tempfile.mkdtemp(prefix="merged_model_")
        print(f"ğŸ“ åˆ›å»ºä¸´æ—¶æ¨¡å‹ç›®å½•: {temp_model_dir}")

        try:
            # å¤åˆ¶åŸºç¡€æ¨¡å‹æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            print("ğŸ“‹ å¤åˆ¶åŸºç¡€æ¨¡å‹æ–‡ä»¶...")
            for item in os.listdir(base_model_path):
                src = os.path.join(base_model_path, item)
                dst = os.path.join(temp_model_dir, item)
                if os.path.isfile(src):
                    shutil.copy2(src, dst)
                elif os.path.isdir(src) and item not in ['.git', '__pycache__']:
                    shutil.copytree(src, dst)

            # å¤åˆ¶RLæƒé‡æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•ï¼Œè¦†ç›–åŸºç¡€æ¨¡å‹æƒé‡
            print("ğŸ“‹ å¤åˆ¶RLæƒé‡æ–‡ä»¶...")
            for item in os.listdir(checkpoint_path):
                if item.endswith('.pt') or item.endswith('.bin') or item.endswith('.safetensors'):
                    src = os.path.join(checkpoint_path, item)
                    # é‡å‘½åactoræƒé‡æ–‡ä»¶ä¸ºæ ‡å‡†åç§°
                    if 'actor' in item.lower():
                        # å°è¯•æ‰¾åˆ°åŸºç¡€æ¨¡å‹çš„æƒé‡æ–‡ä»¶å
                        base_weight_files = [f for f in os.listdir(temp_model_dir)
                                             if f.endswith('.bin') or f.endswith('.safetensors')]
                        if base_weight_files:
                            dst_name = base_weight_files[0]  # ä½¿ç”¨ç›¸åŒçš„æ–‡ä»¶å
                        else:
                            dst_name = 'pytorch_model.bin'  # é»˜è®¤åç§°
                        dst = os.path.join(temp_model_dir, dst_name)
                    else:
                        dst = os.path.join(temp_model_dir, item)

                    print(f"  å¤åˆ¶ {item} -> {os.path.basename(dst)}")
                    shutil.copy2(src, dst)

            # ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ç›®å½•åŠ è½½
            print(f"ğŸ”„ åŠ è½½åˆå¹¶åçš„æ¨¡å‹...")
            llm = LLM(
                model=temp_model_dir,
                tensor_parallel_size=tensor_parallel_size,
                gpu_memory_utilization=gpu_memory_rate,
                trust_remote_code=True
            )

            # ä¿å­˜ä¸´æ—¶ç›®å½•è·¯å¾„ï¼Œä»¥ä¾¿åç»­æ¸…ç†
            llm._temp_model_dir = temp_model_dir

        except Exception as e:
            # å¦‚æœå¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶ç›®å½•å¹¶å›é€€åˆ°åŸºç¡€æ¨¡å‹
            shutil.rmtree(temp_model_dir, ignore_errors=True)
            print(f"âŒ åˆå¹¶æ¨¡å‹å¤±è´¥: {e}")
            print(f"ğŸ”„ å›é€€åˆ°åŸºç¡€æ¨¡å‹: {base_model_path}")
            print("âš ï¸ è­¦å‘Šï¼šå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è€ŒéRLè®­ç»ƒçš„æƒé‡")

            llm = LLM(
                model=base_model_path,
                tensor_parallel_size=tensor_parallel_size,
                gpu_memory_utilization=gpu_memory_rate,
                trust_remote_code=True
            )

    else:
        # æ ‡å‡†åŠ è½½æµç¨‹ï¼ˆcheckpointåŒ…å«å®Œæ•´æ¨¡å‹ï¼‰
        print("âœ… æ£€æµ‹åˆ°å®Œæ•´æ¨¡å‹ç›®å½•ï¼Œä½¿ç”¨æ ‡å‡†åŠ è½½æµç¨‹")
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        llm = LLM(
            model=checkpoint_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_rate,
            trust_remote_code=True
        )

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    return llm, tokenizer


def create_chat_prompt(question: str, tokenizer) -> str:
    """
    åˆ›å»ºèŠå¤©prompt

    Args:
        question: é—®é¢˜
        tokenizer: tokenizer

    Returns:
        æ ¼å¼åŒ–çš„prompt
    """
    messages_chat = [
        {
            "role": "system",
            "content": """You are a helpful assistant.
Given a question, you should answer it by first thinking about the reasoning process in the mind and then providing the final answer.
The output format of reasoning process and final answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., "<think> reasoning process here </think>\n\n<answer> final answer here </answer>".

During the thinking process, you have access to two external tools if necessary:

**Tool 1: Search System**
- Format: "<|begin_of_query|> search query (only list keywords, such as "keyword_1 keyword_2 ...")</|end_of_query|>"
- Function: Retrieves relevant information from external knowledge bases
- Response format: "<|begin_of_documents|> ...search results... </|end_of_documents|>"

**Tool 2: Document Generation System** 
- Format: "<|begin_of_generation|> generation query (only list keywords, such as "keyword_1 keyword_2 ...")</|end_of_generation|>"
- Function: A 7B parameter language model that generates documents tailored to your query
- Response format: "<|begin_of_documents|> ...generated document... </|end_of_documents|>"

**Note**: Each query must involve only a single concept or topic."""
        },
        {"role": "user", "content": question}
    ]

    chat_prompt = tokenizer.apply_chat_template(
        messages_chat,
        tokenize=False,
        add_generation_prompt=True
    )

    return chat_prompt + "<think>"


def call_retrieval_service(queries: List[str], url: str = "http://0.0.0.0:5003/queries", k: int = 3,
                           timeout: int = 300) -> List[List[str]]:
    """
    è°ƒç”¨æ£€ç´¢æœåŠ¡

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        url: æ£€ç´¢æœåŠ¡URL
        k: è¿”å›top-kç»“æœ
        timeout: è¶…æ—¶æ—¶é—´

    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    try:
        response = requests.post(url, json={"queries": queries, "k": k}, timeout=timeout)
        if response.status_code == 200:
            result = response.json()
            return result["answers"]
        else:
            print(f"âš ï¸ æ£€ç´¢æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return [[] for _ in queries]
    except Exception as e:
        print(f"âŒ æ£€ç´¢è¯·æ±‚å¤±è´¥: {e}")
        return [[] for _ in queries]


def call_generation_service(queries: List[str], url: str = "http://101.42.41.82:5004/generate_docs", k: int = 1,
                            timeout: int = 300) -> List[str]:
    """
    è°ƒç”¨æ–‡æ¡£ç”ŸæˆæœåŠ¡

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        url: ç”ŸæˆæœåŠ¡URL
        k: è¿”å›top-kç»“æœ
        timeout: è¶…æ—¶æ—¶é—´

    Returns:
        ç”Ÿæˆæ–‡æ¡£åˆ—è¡¨
    """
    try:
        response = requests.post(url, json={"queries": queries, "k": k}, timeout=timeout)
        if response.status_code == 200:
            result = response.json()
            return result["documents"]
        else:
            print(f"âš ï¸ æ–‡æ¡£ç”ŸæˆæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return ["No document generated." for _ in queries]
    except Exception as e:
        print(f"âŒ æ–‡æ¡£ç”Ÿæˆè¯·æ±‚å¤±è´¥: {e}")
        return ["No document generated." for _ in queries]


def format_retrieval_results(docs: List[str]) -> str:
    """
    æ ¼å¼åŒ–æ£€ç´¢ç»“æœ

    Args:
        docs: æ–‡æ¡£åˆ—è¡¨

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æ¡£å­—ç¬¦ä¸²
    """
    if not docs:
        return "None"

    doc_content_list = []
    for j, doc in enumerate(docs):
        doc_now = re.sub(r'^\d+\s+', '', doc)
        doc_content_list.append(f"({j + 1}){doc_now}\n")

    return ''.join(doc_content_list)


def generate_answer_with_tools(llm, question: str, tokenizer,
                               retrieval_url: str = "http://0.0.0.0:5003/queries",
                               generation_url: str = "http://101.42.41.82:5004/generate_docs",
                               max_rounds: int = 10,
                               temperature: float = 0.0) -> Dict[str, Any]:
    """
    ä½¿ç”¨å·¥å…·ç”Ÿæˆç­”æ¡ˆ

    Args:
        llm: vLLMæ¨¡å‹
        question: é—®é¢˜
        tokenizer: tokenizer
        retrieval_url: æ£€ç´¢æœåŠ¡URL
        generation_url: ç”ŸæˆæœåŠ¡URL
        max_rounds: æœ€å¤§è½®æ•°
        temperature: ç”Ÿæˆæ¸©åº¦

    Returns:
        åŒ…å«ç­”æ¡ˆå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    # åˆ›å»ºåˆå§‹prompt
    current_prompt = create_chat_prompt(question, tokenizer)

    # ç»Ÿè®¡ä¿¡æ¯
    retrieve_count = 0
    generate_count = 0
    round_count = 0
    full_generation = ""

    # åœæ­¢æ ‡è®° - åªåœ¨å·¥å…·è°ƒç”¨æ—¶åœæ­¢ï¼Œè®©æ¨¡å‹è‡ªç„¶ç”Ÿæˆå®Œæ•´å†…å®¹
    stop_tokens = ["</|end_of_query|>", "</|end_of_generation|>"]

    for round_num in range(max_rounds):
        round_count += 1

        # è®¾ç½®é‡‡æ ·å‚æ•° - è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ
        sampling_params = SamplingParams(
            temperature=temperature,  # è¯„ä¼°æ—¶åº”è¯¥æ˜¯0.0
            top_p=1.0,  # è¯„ä¼°æ—¶ä¸ä½¿ç”¨top_pé‡‡æ ·
            max_tokens=512,
            stop=stop_tokens
        )

        # ç”Ÿæˆ
        outputs = llm.generate([current_prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        stop_reason = outputs[0].outputs[0].stop_reason

        full_generation += generated_text

        print(f"ç¬¬{round_num + 1}è½®ç”Ÿæˆ: {generated_text[:100]}...")
        print(f"åœæ­¢åŸå› : {stop_reason}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ£€ç´¢
        if "<|begin_of_query|>" in generated_text and stop_reason == "</|end_of_query|>":
            query = generated_text.split("<|begin_of_query|>")[-1].split("</|end_of_query|>")[0]
            query = query.replace('"', "").strip()
            query = " ".join(query.split())

            if query:
                print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢: {query}")
                retrieve_count += 1

                # è°ƒç”¨æ£€ç´¢æœåŠ¡
                retrieval_results = call_retrieval_service([query], retrieval_url, k=3)
                doc_content = format_retrieval_results(retrieval_results[0] if retrieval_results else [])

                # æ›´æ–°prompt
                current_prompt = (current_prompt + generated_text + "</|end_of_query|>\n\n" +
                                  "<|begin_of_documents|>\n" + doc_content + "</|end_of_documents|>\n\n")
                continue
            else:
                print("âŒ æ£€ç´¢æŸ¥è¯¢ä¸ºç©º")
                break

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ–‡æ¡£
        elif "<|begin_of_generation|>" in generated_text and stop_reason == "</|end_of_generation|>":
            gen_query = generated_text.split("<|begin_of_generation|>")[-1].split("</|end_of_generation|>")[0]
            gen_query = gen_query.strip()

            if gen_query:
                print(f"ğŸ“ ç”ŸæˆæŸ¥è¯¢: {gen_query}")
                generate_count += 1

                # è°ƒç”¨ç”ŸæˆæœåŠ¡
                generation_results = call_generation_service([gen_query], generation_url, k=1)
                generated_doc = generation_results[0] if generation_results else "No document generated."

                # æ›´æ–°prompt
                current_prompt = (current_prompt + generated_text + "</|end_of_generation|>\n\n" +
                                  "<|begin_of_documents|>\n" + generated_doc + "</|end_of_documents|>\n\n")
                continue
            else:
                print("âŒ ç”ŸæˆæŸ¥è¯¢ä¸ºç©º")
                break

        # å…¶ä»–æƒ…å†µï¼šç”Ÿæˆç»“æŸæˆ–åŒ…å«å®Œæ•´ç­”æ¡ˆ
        else:
            print(f"ğŸ’­ ç”Ÿæˆç»“æŸï¼ŒåŸå› : {stop_reason}")
            current_prompt = current_prompt + generated_text
            full_generation += generated_text

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„ç­”æ¡ˆ
            if "<answer>" in full_generation and "</answer>" in full_generation:
                # æå–å®Œæ•´ç­”æ¡ˆ
                answer_part = full_generation.split("<answer>")[-1]
                final_answer = answer_part.split("</answer>")[0].strip()

                return {
                    "question": question,
                    "final_answer": final_answer,
                    "full_generation": full_generation,
                    "retrieve_count": retrieve_count,
                    "generate_count": generate_count,
                    "round_count": round_count,
                    "status": "completed"
                }

            # å¦‚æœåªæœ‰</think>ä½†æ²¡æœ‰ç­”æ¡ˆï¼Œç»§ç»­ç”Ÿæˆç­”æ¡ˆéƒ¨åˆ†
            elif "</think>" in full_generation and "<answer>" not in full_generation:
                print("ğŸ”„ æ€è€ƒå®Œæˆï¼Œç»§ç»­ç”Ÿæˆç­”æ¡ˆ...")
                answer_prompt = current_prompt + "\n\n<answer>"

                # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ - ç¡®å®šæ€§é‡‡æ ·
                final_sampling_params = SamplingParams(
                    temperature=temperature,  # ä¿æŒä¸€è‡´
                    top_p=1.0,
                    max_tokens=256,
                    stop=["</answer>"]
                )

                final_outputs = llm.generate([answer_prompt], final_sampling_params)
                final_answer = final_outputs[0].outputs[0].text.strip()
                full_generation += "\n\n<answer>" + final_answer + "</answer>"

                return {
                    "question": question,
                    "final_answer": final_answer,
                    "full_generation": full_generation,
                    "retrieve_count": retrieve_count,
                    "generate_count": generate_count,
                    "round_count": round_count,
                    "status": "completed"
                }

            # å…¶ä»–æƒ…å†µï¼šä¸å®Œæ•´çš„ç”Ÿæˆ
            else:
                final_answer = "Generation incomplete or malformed."
                return {
                    "question": question,
                    "final_answer": final_answer,
                    "full_generation": full_generation,
                    "retrieve_count": retrieve_count,
                    "generate_count": generate_count,
                    "round_count": round_count,
                    "status": "incomplete"
                }

    # è¶…è¿‡æœ€å¤§è½®æ•°
    print("âš ï¸ è¶…è¿‡æœ€å¤§è½®æ•°é™åˆ¶")
    return {
        "question": question,
        "final_answer": "Max rounds exceeded.",
        "full_generation": full_generation,
        "retrieve_count": retrieve_count,
        "generate_count": generate_count,
        "round_count": round_count,
        "status": "max_rounds_exceeded"
    }


def normalize_answer(text: str) -> str:
    """
    æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬ï¼Œç”¨äºè®¡ç®—EM
    """
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = ' '.join(text.split())
    return text


def calculate_cover_em(predictions: List[str], references: List[str]) -> Dict[str, Any]:
    """
    è®¡ç®—Cover EMå¾—åˆ†

    Args:
        predictions: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
        references: å‚è€ƒç­”æ¡ˆåˆ—è¡¨

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    assert len(predictions) == len(references), "é¢„æµ‹å’Œå‚è€ƒç­”æ¡ˆæ•°é‡ä¸ä¸€è‡´"

    cover_matches = 0
    exact_matches = 0
    detailed_results = []

    for pred, ref in zip(predictions, references):
        norm_pred = normalize_answer(pred)
        norm_ref = normalize_answer(ref)

        # Cover EM: æ£€æŸ¥è¦†ç›–å…³ç³»
        is_cover_match = (norm_pred in norm_ref) or (norm_ref in norm_pred)

        # Exact EM: å®Œå…¨åŒ¹é…
        is_exact_match = norm_pred == norm_ref

        if is_cover_match:
            cover_matches += 1
        if is_exact_match:
            exact_matches += 1

        detailed_results.append({
            'prediction': pred,
            'reference': ref,
            'normalized_prediction': norm_pred,
            'normalized_reference': norm_ref,
            'cover_match': is_cover_match,
            'exact_match': is_exact_match
        })

    total = len(predictions)
    cover_em_score = cover_matches / total
    exact_em_score = exact_matches / total

    return {
        'cover_em_score': cover_em_score,
        'exact_em_score': exact_em_score,
        'total_questions': total,
        'cover_matches': cover_matches,
        'exact_matches': exact_matches,
        'detailed_results': detailed_results
    }


def worker_evaluate(worker_id: int, data_chunk: List[Dict], args, results_queue):
    """
    å•ä¸ªworkerè¿›ç¨‹çš„è¯„ä¼°å‡½æ•°

    Args:
        worker_id: workerç¼–å·
        data_chunk: åˆ†é…ç»™è¿™ä¸ªworkerçš„æ•°æ®å—
        args: å‚æ•°
        results_queue: ç»“æœé˜Ÿåˆ—
    """
    try:
        # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
        gpu_id = worker_id % torch.cuda.device_count()  # å¾ªç¯åˆ†é…GPU
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

        print(f"ğŸš€ Worker {worker_id} å¯åŠ¨ï¼Œä½¿ç”¨GPU {gpu_id}ï¼Œå¤„ç† {len(data_chunk)} ä¸ªæ ·æœ¬")

        # åŠ è½½æ¨¡å‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼‰
        llm, tokenizer = load_rl_model(
            args.checkpoint_path,
            base_model_path=args.base_model_path,
            gpu_memory_rate=args.gpu_memory_rate,
            tensor_parallel_size=1  # æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨1å¼ å¡
        )

        worker_results = []

        # å¤„ç†åˆ†é…ç»™è¿™ä¸ªworkerçš„æ•°æ®
        for i, item in enumerate(data_chunk):
            question = item['question']
            reference = item['answer']

            if i % 10 == 0:  # æ¯10ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"Worker {worker_id}: å¤„ç†è¿›åº¦ {i + 1}/{len(data_chunk)}")

            # ç”Ÿæˆç­”æ¡ˆ
            result = generate_answer_with_tools(
                llm, question, tokenizer,
                retrieval_url=args.retrieval_url,
                generation_url=args.generation_url,
                temperature=args.temperature
            )

            worker_results.append({
                'worker_id': worker_id,
                'question_id': f"{worker_id}_{i}",
                'question': question,
                'reference': reference,
                'prediction': result['final_answer'],
                'full_generation': result['full_generation'],
                'retrieve_count': result['retrieve_count'],
                'generate_count': result['generate_count'],
                'round_count': result['round_count'],
                'status': result['status']
            })

        # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
        results_queue.put(worker_results)
        print(f"âœ… Worker {worker_id} å®Œæˆï¼Œå¤„ç†äº† {len(data_chunk)} ä¸ªæ ·æœ¬")

    except Exception as e:
        print(f"âŒ Worker {worker_id} å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        results_queue.put([])  # æ”¾å…¥ç©ºç»“æœé¿å…ä¸»è¿›ç¨‹ç­‰å¾…


def evaluate_model_parallel(test_file: str, args, num_workers: int = 4) -> Dict[str, Any]:
    """
    å¹¶è¡Œè¯„ä¼°æ¨¡å‹

    Args:
        test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
        args: å‚æ•°å¯¹è±¡
        num_workers: å¹¶è¡Œworkeræ•°é‡

    Returns:
        è¯„ä¼°ç»“æœ
    """
    print(f"ğŸš€ å¼€å§‹å¹¶è¡Œè¯„ä¼°ï¼Œä½¿ç”¨ {num_workers} ä¸ªworkerè¿›ç¨‹")
    print(f"æµ‹è¯•æ–‡ä»¶: {test_file}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    dataset = TestDataset(test_file)
    if args.max_samples > 0:
        dataset.data = dataset.data[:args.max_samples]

    total_samples = len(dataset)
    print(f"æ€»å…± {total_samples} ä¸ªæµ‹è¯•æ ·æœ¬")

    # å°†æ•°æ®åˆ†å—ç»™ä¸åŒçš„worker
    chunk_size = (total_samples + num_workers - 1) // num_workers
    data_chunks = []

    for i in range(num_workers):
        start_idx = i * chunk_size
        end_idx = min(start_idx + chunk_size, total_samples)
        if start_idx < total_samples:
            chunk = dataset.data[start_idx:end_idx]
            data_chunks.append(chunk)
            print(f"Worker {i}: æ ·æœ¬ {start_idx}-{end_idx - 1} ({len(chunk)} ä¸ª)")

    # åˆ›å»ºç»“æœé˜Ÿåˆ—
    results_queue = mp.Queue()

    # å¯åŠ¨workerè¿›ç¨‹
    processes = []
    for i, chunk in enumerate(data_chunks):
        p = mp.Process(
            target=worker_evaluate,
            args=(i, chunk, args, results_queue)
        )
        p.start()
        processes.append(p)

    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_worker_results = []
    for i in range(len(data_chunks)):
        worker_results = results_queue.get()
        all_worker_results.extend(worker_results)
        print(f"âœ… æ”¶åˆ°workerç»“æœï¼Œå½“å‰æ€»æ•°: {len(all_worker_results)}")

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()

    print(f"ğŸ‰ æ‰€æœ‰workerå®Œæˆï¼æ€»å…±å¤„ç† {len(all_worker_results)} ä¸ªæ ·æœ¬")

    # æ•´ç†ç»“æœ
    predictions = []
    references = []
    detailed_results = []

    # ç»Ÿè®¡ä¿¡æ¯
    total_retrieve_count = 0
    total_generate_count = 0
    status_counts = defaultdict(int)

    for result in all_worker_results:
        predictions.append(result['prediction'])
        references.append(result['reference'])
        detailed_results.append(result)

        total_retrieve_count += result['retrieve_count']
        total_generate_count += result['generate_count']
        status_counts[result['status']] += 1

    # è®¡ç®—Cover EMç­‰æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    em_results = calculate_cover_em(predictions, references)

    # æ±‡æ€»ç»“æœ
    evaluation_results = {
        'metrics': em_results,
        'statistics': {
            'total_samples': len(all_worker_results),
            'avg_retrieve_count': total_retrieve_count / len(all_worker_results) if all_worker_results else 0,
            'avg_generate_count': total_generate_count / len(all_worker_results) if all_worker_results else 0,
            'total_retrieve_count': total_retrieve_count,
            'total_generate_count': total_generate_count,
            'status_distribution': dict(status_counts),
            'num_workers_used': len(data_chunks)
        },
        'detailed_results': detailed_results
    }

    return evaluation_results


def evaluate_model_single(llm, tokenizer, test_file: str,
                          retrieval_url: str = "http://0.0.0.0:5003/queries",
                          generation_url: str = "http://101.42.41.82:5004/generate_docs",
                          max_samples: int = -1,
                          temperature: float = 0.0) -> Dict[str, Any]:
    """
    å•è¿›ç¨‹è¯„ä¼°æ¨¡å‹

    Args:
        llm: vLLMæ¨¡å‹
        tokenizer: tokenizer
        test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
        retrieval_url: æ£€ç´¢æœåŠ¡URL
        generation_url: ç”ŸæˆæœåŠ¡URL
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨
        temperature: ç”Ÿæˆæ¸©åº¦

    Returns:
        è¯„ä¼°ç»“æœ
    """
    print(f"å¼€å§‹å•è¿›ç¨‹è¯„ä¼°ï¼Œæµ‹è¯•æ–‡ä»¶: {test_file}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    dataset = TestDataset(test_file)
    if max_samples > 0:
        dataset.data = dataset.data[:max_samples]

    print(f"æ€»å…± {len(dataset)} ä¸ªæµ‹è¯•æ ·æœ¬")

    predictions = []
    references = []
    detailed_results = []

    # ç»Ÿè®¡ä¿¡æ¯
    total_retrieve_count = 0
    total_generate_count = 0
    status_counts = defaultdict(int)

    print("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
    for i, item in enumerate(tqdm(dataset.data)):
        question = item['question']
        reference = item['answer']

        if i < 3:  # åªä¸ºå‰3ä¸ªæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            print(f"\n{'=' * 60}")
            print(f"å¤„ç†ç¬¬ {i + 1}/{len(dataset)} ä¸ªé—®é¢˜")
            print(f"é—®é¢˜: {question}")
            print(f"å‚è€ƒç­”æ¡ˆ: {reference}")

        # ç”Ÿæˆç­”æ¡ˆ
        result = generate_answer_with_tools(
            llm, question, tokenizer,
            retrieval_url=retrieval_url,
            generation_url=generation_url,
            temperature=temperature
        )

        prediction = result['final_answer']
        predictions.append(prediction)
        references.append(reference)

        # ç»Ÿè®¡ä¿¡æ¯
        total_retrieve_count += result['retrieve_count']
        total_generate_count += result['generate_count']
        status_counts[result['status']] += 1

        detailed_results.append({
            'question_id': i,
            'question': question,
            'reference': reference,
            'prediction': prediction,
            'full_generation': result['full_generation'],
            'retrieve_count': result['retrieve_count'],
            'generate_count': result['generate_count'],
            'round_count': result['round_count'],
            'status': result['status']
        })

        if i < 3:  # åªä¸ºå‰3ä¸ªæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            print(f"é¢„æµ‹ç­”æ¡ˆ: {prediction}")
            print(f"æ£€ç´¢æ¬¡æ•°: {result['retrieve_count']}, ç”Ÿæˆæ¬¡æ•°: {result['generate_count']}")
            print(f"çŠ¶æ€: {result['status']}")
            print(f"å®Œæ•´ç”Ÿæˆè¿‡ç¨‹:\n{result['full_generation']}")

    # è®¡ç®—Cover EMç­‰æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    em_results = calculate_cover_em(predictions, references)

    # æ±‡æ€»ç»“æœ
    evaluation_results = {
        'metrics': em_results,
        'statistics': {
            'total_samples': len(dataset),
            'avg_retrieve_count': total_retrieve_count / len(dataset),
            'avg_generate_count': total_generate_count / len(dataset),
            'total_retrieve_count': total_retrieve_count,
            'total_generate_count': total_generate_count,
            'status_distribution': dict(status_counts)
        },
        'detailed_results': detailed_results
    }

    return evaluation_results


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°RLè®­ç»ƒçš„LLMæ¨¡å‹ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰')
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--test_file', type=str, required=True,
                        help='æµ‹è¯•æ–‡ä»¶è·¯å¾„(jsonlæ ¼å¼)')
    parser.add_argument('--output_file', type=str, default='evaluation_results.json',
                        help='ç»“æœè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--retrieval_url', type=str, default='http://0.0.0.0:5003/queries',
                        help='æ£€ç´¢æœåŠ¡URL')
    parser.add_argument('--generation_url', type=str, default='http://101.42.41.82:5004/generate_docs',
                        help='æ–‡æ¡£ç”ŸæˆæœåŠ¡URL')
    parser.add_argument('--gpu_memory_rate', type=float, default=0.9,
                        help='GPUå†…å­˜ä½¿ç”¨ç‡')
    parser.add_argument('--max_samples', type=int, default=-1,
                        help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨')
    parser.add_argument('--num_workers', type=int, default=1,
                        help='å¹¶è¡Œworkeræ•°é‡ï¼ˆ1=å•è¿›ç¨‹ï¼Œ>1=å¤šè¿›ç¨‹å¹¶è¡Œï¼‰')
    parser.add_argument('--tensor_parallel_size', type=int, default=1,
                        help='å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå•è¿›ç¨‹æ¨¡å¼ä¸‹ä½¿ç”¨çš„GPUæ•°é‡ï¼‰')
    parser.add_argument('--temperature', type=float, default=0.0,
                        help='ç”Ÿæˆæ¸©åº¦ï¼ˆè¯„ä¼°æ—¶å»ºè®®ä½¿ç”¨0.0ï¼‰')
    parser.add_argument('--base_model_path', type=str, default=None,
                        help='åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå½“checkpointåªåŒ…å«æƒé‡æ—¶ä½¿ç”¨ï¼‰')

    args = parser.parse_args()

    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

    available_gpus = torch.cuda.device_count()
    print(f"å¯ç”¨GPUæ•°é‡: {available_gpus}")

    if args.num_workers > available_gpus:
        print(f"âš ï¸ è­¦å‘Šï¼šworkeræ•°é‡({args.num_workers})è¶…è¿‡å¯ç”¨GPUæ•°é‡({available_gpus})")
        print(f"å»ºè®®è®¾ç½® --num_workers {available_gpus}")

    print(f"å¼€å§‹è¯„ä¼°...")
    print(f"æ¨¡å‹è·¯å¾„: {args.checkpoint_path}")
    print(f"æµ‹è¯•æ–‡ä»¶: {args.test_file}")
    print(f"æ£€ç´¢æœåŠ¡: {args.retrieval_url}")
    print(f"ç”ŸæˆæœåŠ¡: {args.generation_url}")

    if args.num_workers > 1:
        print(f"ğŸš€ å¹¶è¡Œæ¨¡å¼: {args.num_workers} ä¸ªworkerè¿›ç¨‹ï¼Œæ¯ä¸ªä½¿ç”¨1å¼ GPU")
        print(f"é¢„è®¡åŠ é€Ÿæ¯”: ~{args.num_workers}x")
    else:
        print(f"ğŸ”§ å•è¿›ç¨‹æ¨¡å¼: {args.tensor_parallel_size}å¼ å¡å¹¶è¡Œï¼Œå†…å­˜åˆ©ç”¨ç‡{args.gpu_memory_rate}")

    print(f"ç”Ÿæˆæ¸©åº¦: {args.temperature} {'âœ… ç¡®å®šæ€§ç”Ÿæˆ' if args.temperature == 0.0 else 'âš ï¸ éç¡®å®šæ€§ç”Ÿæˆ'}")

    llm = None  # åˆå§‹åŒ–llmå˜é‡

    try:
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¯„ä¼°æ–¹æ³•
        if args.num_workers > 1:
            # å¤šè¿›ç¨‹å¹¶è¡Œæ¨¡å¼
            print(f"\nğŸš€ å¯åŠ¨å¹¶è¡Œè¯„ä¼°æ¨¡å¼...")

            results = evaluate_model_parallel(
                args.test_file,
                args,
                num_workers=args.num_workers
            )
        else:
            # å•è¿›ç¨‹æ¨¡å¼
            print(f"\nğŸ”§ å¯åŠ¨å•è¿›ç¨‹è¯„ä¼°æ¨¡å¼...")

            # åŠ è½½æ¨¡å‹
            llm, tokenizer = load_rl_model(
                args.checkpoint_path,
                base_model_path=args.base_model_path,
                gpu_memory_rate=args.gpu_memory_rate,
                tensor_parallel_size=args.tensor_parallel_size
            )

            # è¯„ä¼°æ¨¡å‹
            results = evaluate_model_single(
                llm, tokenizer, args.test_file,
                retrieval_url=args.retrieval_url,
                generation_url=args.generation_url,
                max_samples=args.max_samples,
                temperature=args.temperature
            )

        # æ‰“å°ç»“æœ
        print(f"\n{'=' * 80}")
        print("ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"{'=' * 80}")
        print(f"Cover EMå¾—åˆ†: {results['metrics']['cover_em_score']:.4f}")
        print(f"Exact EMå¾—åˆ†: {results['metrics']['exact_em_score']:.4f}")
        print(f"æ€»é—®é¢˜æ•°: {results['metrics']['total_questions']}")
        print(f"CoveråŒ¹é…æ•°: {results['metrics']['cover_matches']}")
        print(f"ExactåŒ¹é…æ•°: {results['metrics']['exact_matches']}")

        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"å¹³å‡æ£€ç´¢æ¬¡æ•°: {results['statistics']['avg_retrieve_count']:.2f}")
        print(f"å¹³å‡ç”Ÿæˆæ¬¡æ•°: {results['statistics']['avg_generate_count']:.2f}")
        print(f"æ€»æ£€ç´¢æ¬¡æ•°: {results['statistics']['total_retrieve_count']}")
        print(f"æ€»ç”Ÿæˆæ¬¡æ•°: {results['statistics']['total_generate_count']}")

        if 'num_workers_used' in results['statistics']:
            print(f"ä½¿ç”¨workeræ•°é‡: {results['statistics']['num_workers_used']}")

        print(f"\nğŸ“‹ çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in results['statistics']['status_distribution'].items():
            print(f"  {status}: {count} ({count / results['statistics']['total_samples'] * 100:.1f}%)")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(args.output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")

    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        cleanup_temp_dirs(llm)


def cleanup_temp_dirs(llm=None):
    """æ¸…ç†ä¸´æ—¶æ¨¡å‹ç›®å½•"""
    if llm and hasattr(llm, '_temp_model_dir'):
        import shutil
        temp_dir = llm._temp_model_dir
        if os.path.exists(temp_dir):
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            shutil.rmtree(temp_dir, ignore_errors=True)

if __name__ == '__main__':
    main()