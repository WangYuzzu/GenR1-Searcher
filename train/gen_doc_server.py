# gendoc_server.py
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "9"

import torch
import argparse
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List
import logging
import time
from datetime import datetime
import requests

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GenDocModel:
    def __init__(self, model_path: str, wiki_service_url: str = "http://101.42.41.82:5004"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.wiki_service_url = wiki_service_url  # æ·»åŠ wikiæœåŠ¡URL

        logger.info(f"ğŸš€ åŠ è½½GenDocæ¨¡å‹: {model_path} åˆ° {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        self.model.eval()
        logger.info(f"âœ… GenDocæ¨¡å‹åŠ è½½å®Œæˆï¼Œæ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024 ** 3:.1f}GB")

        # ç”Ÿæˆå‚æ•°
        self.generation_config = {
            "max_new_tokens": 512,
            "do_sample": False,
            "pad_token_id": self.tokenizer.eos_token_id,
        }

    # def _retrieve_wiki_documents(self, queries: List[str], k: int = 1) -> List[List[str]]:
    #     """ä»wikiè¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    #     try:
    #         response = requests.post(
    #             f"{self.wiki_service_url}/queries",
    #             json={"queries": queries, "k": k},
    #             timeout=120
    #         )
    #
    #         if response.status_code == 200:
    #             result = response.json()
    #             return result.get("answers", [[] for _ in queries])
    #         else:
    #             logger.error(f"Wikiæ£€ç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    #             return [[] for _ in queries]
    #
    #     except Exception as e:
    #         logger.error(f"Wikiæ£€ç´¢æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
    #         return [[] for _ in queries]
    def _retrieve_wiki_documents(self, queries: List[str], k: int = 1) -> List[List[str]]:
        """ä»wikiè¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'User-Agent': 'GenDocServer/1.0'  # æ·»åŠ  User-Agent
            }

            response = requests.post(
                f"{self.wiki_service_url}/queries",
                json={"queries": queries, "k": k},
                headers=headers,  # æ·»åŠ è¯·æ±‚å¤´
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("answers", [[] for _ in queries])
            else:
                logger.error(f"Wikiæ£€ç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return [[] for _ in queries]

        except Exception as e:
            logger.error(f"Wikiæ£€ç´¢æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return [[] for _ in queries]

    def generate_documents(self, queries: List[str], k: int = 3) -> List[str]:
        """å…¨å¹¶è¡Œç”Ÿæˆæ–‡æ¡£ - æ‰€æœ‰queryÃ—kä¸ªæ–‡æ¡£ä¸€æ¬¡æ€§ç”Ÿæˆ"""
        total_docs = len(queries) * k
        logger.info(f"ğŸ“ å°è¯•å¹¶è¡Œç”Ÿæˆ {len(queries)} ä¸ªæŸ¥è¯¢ Ã— {k} ä¸ªæ–‡æ¡£ = {total_docs} ä¸ªæ–‡æ¡£")

        try:
            # å°è¯•å…¨å¹¶è¡Œç”Ÿæˆ
            return self._generate_all_parallel(queries, k)

        except torch.cuda.OutOfMemoryError:
            logger.warning("âš ï¸ å…¨å¹¶è¡Œç”Ÿæˆæ˜¾å­˜ä¸è¶³ï¼Œå›é€€åˆ°åˆ†æ‰¹å¤„ç†")
            torch.cuda.empty_cache()
            return self._generate_batch_fallback(queries, k)

        except Exception as e:
            logger.error(f"âŒ å…¨å¹¶è¡Œç”Ÿæˆå¤±è´¥: {e}ï¼Œå›é€€åˆ°åˆ†æ‰¹å¤„ç†")
            return self._generate_batch_fallback(queries, k)

    def _generate_all_parallel(self, queries: List[str], k: int) -> List[str]:
        """å°è¯•å…¨å¹¶è¡Œç”Ÿæˆæ‰€æœ‰æ–‡æ¡£"""

        # å…ˆæ£€ç´¢æ¯ä¸ªqueryçš„å‚è€ƒæ–‡æ¡£
        logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢ {len(queries)} ä¸ªæŸ¥è¯¢çš„å‚è€ƒæ–‡æ¡£...")
        reference_docs = self._retrieve_wiki_documents(queries, k=1)  # æ¯ä¸ªqueryæ£€ç´¢1ä¸ªå‚è€ƒæ–‡æ¡£

        # 1. æ„å»ºæ‰€æœ‰prompts (NÃ—kä¸ª)
        all_prompts = []
        query_indices = []  # è®°å½•æ¯ä¸ªpromptå¯¹åº”çš„queryç´¢å¼•

        for query_idx, (query, ref_docs) in enumerate(zip(queries, reference_docs)):
            # è·å–è¯¥queryçš„å‚è€ƒæ–‡æ¡£ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            reference_doc = ref_docs[0] if ref_docs else None
            prompt = self._build_prompt(query, reference_doc)

            for doc_idx in range(k):
                all_prompts.append(prompt)
                query_indices.append(query_idx)

        logger.info(f"ğŸš€ æ„å»ºäº† {len(all_prompts)} ä¸ªpromptsï¼Œå¼€å§‹æ‰¹é‡ç”Ÿæˆ...")

        # 2. æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
        if not self._check_memory_sufficient(len(all_prompts)):
            raise torch.cuda.OutOfMemoryError("é¢„ä¼°æ˜¾å­˜ä¸è¶³")

        # 3. æ‰¹é‡tokenizeæ‰€æœ‰prompts
        inputs = self.tokenizer(
            all_prompts,
            return_tensors="pt",
            max_length=1024,
            truncation=True,
            padding=True
        ).to(self.device)

        # 4. æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                **self.generation_config
            )

        # 5. æ‰¹é‡è§£ç 
        generated_texts = self.tokenizer.batch_decode(
            outputs[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_texts)} ä¸ªæ–‡æ¡£")

        # 6. é‡æ–°ç»„ç»‡ç»“æœ - æŒ‰queryåˆ†ç»„
        results = self._reorganize_results(queries, generated_texts, query_indices, k)

        # âœ… æ‰“å°æ¯ä¸ª query çš„æ–‡æ¡£æ€»è¯æ•°
        for query_idx, merged_doc in enumerate(results):
            total_words = len(merged_doc.split())
            logger.info(f"ğŸ“Š Query {query_idx + 1} æ€»è¯æ•°: {total_words}")

        return results

    def _check_memory_sufficient(self, num_prompts: int) -> bool:
        """æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿè¿›è¡Œå…¨å¹¶è¡Œç”Ÿæˆ"""
        if not torch.cuda.is_available():
            return False

        # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated()
        free_memory = total_memory - allocated_memory
        free_gb = free_memory / (1024 ** 3)

        # ä¼°ç®—éœ€è¦çš„é¢å¤–æ˜¾å­˜ (å¾ˆç²—ç•¥çš„ä¼°è®¡)
        # å‡è®¾æ¯ä¸ªpromptåœ¨ç”Ÿæˆæ—¶éœ€è¦é¢å¤–çš„æ˜¾å­˜
        estimated_need_gb = num_prompts * 0.2  # æ¯ä¸ªpromptå¤§çº¦éœ€è¦200MB

        logger.info(f"ğŸ” æ˜¾å­˜æ£€æŸ¥: å¯ç”¨={free_gb:.1f}GB, é¢„ä¼°éœ€è¦={estimated_need_gb:.1f}GB")

        return free_gb > estimated_need_gb * 1.2  # ç•™20%å®‰å…¨è¾¹è·

    def _reorganize_results(self, queries: List[str], generated_texts: List[str],
                            query_indices: List[int], k: int) -> List[str]:
        """å°†ç”Ÿæˆçš„æ–‡æ¡£æŒ‰queryé‡æ–°ç»„ç»‡"""
        results = [[] for _ in queries]

        # å°†æ¯ä¸ªç”Ÿæˆçš„æ–‡æ¡£åˆ†é…åˆ°å¯¹åº”çš„query
        for i, (text, query_idx) in enumerate(zip(generated_texts, query_indices)):
            if text.strip():
                doc_num = (i % k) + 1  # æ–‡æ¡£ç¼–å· 1, 2, 3, ...
                formatted_doc = f"Document {doc_num}: {text.strip()}"
                results[query_idx].append(formatted_doc)

        # åˆå¹¶æ¯ä¸ªqueryçš„æ‰€æœ‰æ–‡æ¡£
        final_results = []
        for query_idx, query_docs in enumerate(results):
            if query_docs:
                merged_doc = "\n\n".join(query_docs)
            else:
                merged_doc = f"No relevant document could be generated for query: {queries[query_idx]}"
            final_results.append(merged_doc)

        return final_results

    def _generate_batch_fallback(self, queries: List[str], k: int) -> List[str]:
        """å›é€€æ–¹æ¡ˆï¼šåˆ†æ‰¹å¤„ç†"""
        results = []

        # åŠ¨æ€è®¡ç®—å®‰å…¨çš„batch size
        batch_size = self._calculate_safe_batch_size(len(queries), k)
        logger.info(f"ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼Œbatch_size={batch_size}")

        for i in range(0, len(queries), batch_size):
            batch_queries = queries[i:i + batch_size]

            try:
                # å°è¯•å¯¹è¿™ä¸ªbatchè¿›è¡Œå¹¶è¡Œå¤„ç†
                batch_results = self._generate_all_parallel(batch_queries, k)
                results.extend(batch_results)

            except (torch.cuda.OutOfMemoryError, Exception):
                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå›é€€åˆ°æœ€ä¿å®ˆçš„é€ä¸ªå¤„ç†
                logger.warning(f"âš ï¸ batch {i // batch_size + 1} ä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨é€ä¸ªå¤„ç†")
                torch.cuda.empty_cache()

                for query in batch_queries:
                    query_result = self._generate_single_query_documents(query, k)
                    results.append(query_result)

            # æ¯ä¸ªbatchåæ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return results

    def _generate_single_query_documents(self, query: str, k: int) -> str:
        """æœ€ä¿å®ˆçš„æ–¹æ¡ˆï¼šä¸ºå•ä¸ªqueryé¡ºåºç”Ÿæˆkä¸ªæ–‡æ¡£"""
        # å…ˆæ£€ç´¢å‚è€ƒæ–‡æ¡£
        reference_docs = self._retrieve_wiki_documents([query], k=1)
        reference_doc = reference_docs[0][0] if reference_docs and reference_docs[0] else None

        prompt = self._build_prompt(query, reference_doc)
        docs = []

        for i in range(k):
            try:
                doc = self._generate_single_document(prompt)
                if doc.strip():
                    docs.append(f"Document {i + 1}: {doc.strip()}")
            except Exception as e:
                logger.error(f"âŒ æ–‡æ¡£ {i + 1} ç”Ÿæˆå¤±è´¥: {e}")

        return "\n\n".join(docs) if docs else f"Failed to generate documents for query: {query}"

    def _calculate_safe_batch_size(self, num_queries: int, k: int) -> int:
        """æ ¹æ®å¯ç”¨æ˜¾å­˜è®¡ç®—å®‰å…¨çš„batch size"""
        if not torch.cuda.is_available():
            return 1

        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()
        free_gb = free_memory / (1024 ** 3)

        # æ›´æ¿€è¿›çš„batch sizeè®¡ç®—
        if free_gb > 10:
            return min(8, num_queries)  # æ˜¾å­˜å……è¶³ï¼šæœ€å¤š8ä¸ªqueries
        elif free_gb > 6:
            return min(4, num_queries)  # æ˜¾å­˜è¾ƒå¥½ï¼šæœ€å¤š4ä¸ªqueries
        elif free_gb > 3:
            return min(2, num_queries)  # æ˜¾å­˜ç´§å¼ ï¼šæœ€å¤š2ä¸ªqueries
        else:
            return 1  # æ˜¾å­˜ä¸è¶³ï¼šé€ä¸ªå¤„ç†

    def _build_prompt(self, query: str, reference_doc: str = None) -> str:
        """æ„å»ºç”Ÿæˆæ–‡æ¡£çš„æç¤ºè¯"""
        system_content = "You are a document generator. Generate a background document from Wikipedia to answer the given question."

        if reference_doc:
            system_content += f"\nHere are some references that may be relevant:\n{reference_doc}"
        else:
            system_content += " Output only the document content."

        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user",
                "content": query
            }
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        return prompt

    def _generate_single_document(self, prompt: str) -> str:
        """ç”Ÿæˆå•ä¸ªæ–‡æ¡£"""
        try:
            # Tokenizeè¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=1024,
                truncation=True
            ).to(self.device)

            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **self.generation_config
                )

            # è§£ç è¾“å‡º
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"âŒ å•æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def get_memory_usage(self):
        """è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            return {
                "allocated": f"{torch.cuda.memory_allocated() / 1024 ** 3:.1f}GB",
                "cached": f"{torch.cuda.memory_reserved() / 1024 ** 3:.1f}GB"
            }
        return {"status": "CUDA not available"}


class GenDocServer:
    def __init__(self, model_path: str, wiki_service_url: str = "http://101.42.41.82:5004",
                 host: str = "0.0.0.0", port: int = 5004):
        self.host = host
        self.port = port

        # ç›´æ¥åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸ä½¿ç”¨Rayï¼‰
        logger.info("ğŸš€ åˆå§‹åŒ–GenDocæ¨¡å‹...")
        self.gendoc_model = GenDocModel(model_path, wiki_service_url)

        # é¢„çƒ­æ¨¡å‹
        logger.info("ğŸ”¥ é¢„çƒ­GenDocæ¨¡å‹...")
        test_result = self.gendoc_model.generate_documents(["test query"], k=1)
        logger.info(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ: {test_result[0][:100]}...")

        # åˆ›å»ºFastAPIåº”ç”¨
        self.app = FastAPI(title="GenDoc Server")
        self._setup_routes()

    def _setup_routes(self):
        @self.app.post("/generate_docs")
        async def generate_docs(request: Request):
            try:
                data = await request.json()
                queries = data.get("queries", [])
                k = data.get("k", 3)  # é»˜è®¤ç”Ÿæˆ3ä¸ªæ–‡æ¡£
                print(f'ç”Ÿæˆ{k}ä¸ªæ–‡æ¡£')

                if not queries:
                    return JSONResponse({"error": "No queries provided"}, status_code=400)

                logger.info(f"ğŸ“¥ æ”¶åˆ° {len(queries)} ä¸ªç”Ÿæˆè¯·æ±‚")
                start_time = time.time()

                # ç›´æ¥è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ–‡æ¡£ï¼ˆä¸ä½¿ç”¨Rayï¼‰
                documents = self.gendoc_model.generate_documents(queries, k)

                end_time = time.time()
                logger.info(f"ğŸ“¤ ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
                print(f'ç”Ÿæˆæ–‡æ¡£å¦‚ä¸‹: {documents}')

                return JSONResponse({
                    "documents": documents,
                    "count": len(documents),
                    "generation_time": f"{end_time - start_time:.2f}s"
                })

            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.app.get("/health")
        async def health_check():
            try:
                memory_info = self.gendoc_model.get_memory_usage()
                return JSONResponse({
                    "status": "healthy",
                    "memory": memory_info,
                    "timestamp": datetime.now().isoformat()
                })
            except Exception as e:
                return JSONResponse({"status": "unhealthy", "error": str(e)})

        @self.app.get("/")
        async def root():
            return JSONResponse({
                "message": "GenDoc Server is running",
                "endpoints": {
                    "generate_docs": "POST /generate_docs - ç”Ÿæˆæ–‡æ¡£",
                    "health": "GET /health - å¥åº·æ£€æŸ¥"
                }
            })

    def run(self):
        logger.info(f"ğŸŒ å¯åŠ¨GenDocæœåŠ¡å™¨ http://{self.host}:{self.port}")
        uvicorn.run(
            self.app,
            host=self.host,
            port=self.port,
            log_level="info"
        )


def main():
    parser = argparse.ArgumentParser(description="GenDoc Server")
    parser.add_argument("--model_path", type=str, default="/root/autodl-tmp/Qwen-2.5-7B-Instruct")
    parser.add_argument("--wiki_service_url", type=str, default="http://101.42.41.82:5004",
                        help="Wikiæ£€ç´¢æœåŠ¡çš„URL")
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int, default=5004)

    args = parser.parse_args()

    try:
        # ç›´æ¥åˆå§‹åŒ–å¹¶å¯åŠ¨æœåŠ¡å™¨ï¼ˆä¸ä½¿ç”¨Rayï¼‰
        server = GenDocServer(args.model_path, args.wiki_service_url, args.host, args.port)
        server.run()

    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        exit(1)


if __name__ == "__main__":
    main()
